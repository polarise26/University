{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 11 - BERT Implementation\n",
    "___\n",
    "\n",
    "Using hugging face pretrained models.  \n",
    "BERT stands for Bidirectional Encoder Representations from Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kannu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glw\n"
     ]
    }
   ],
   "source": [
    "sample = input(\"Enter a string: \")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input text\n",
    "tokens = tokenizer.encode(sample, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized sequence to PyTorch tensors\n",
    "tokens_tensor = torch.tensor(tokens).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input through BERT model to get the outputs\n",
    "outputs = model(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2247e-01,  1.0307e-01,  2.5152e-01, -1.2580e-01, -1.2775e-01,\n",
      "         -5.5913e-02,  1.1254e-01,  1.9855e-01, -7.8039e-02, -1.1481e-01,\n",
      "         -1.9383e-01,  1.4925e-01, -3.0677e-02,  5.8242e-01,  1.6761e-01,\n",
      "          2.0463e-01, -3.6064e-01,  7.1997e-01,  2.4431e-01, -3.2773e-01,\n",
      "         -1.0900e-01, -3.3103e-01,  1.8216e-02, -2.5717e-01, -6.3992e-02,\n",
      "          3.2706e-02, -3.0477e-01, -1.5038e-01,  1.9739e-02,  6.6883e-02,\n",
      "         -1.1199e-01,  2.6141e-01, -1.6487e-01,  1.5882e-01,  1.8117e-01,\n",
      "         -5.7753e-02,  9.7379e-03, -2.1664e-01, -2.8773e-02,  3.6730e-01,\n",
      "          1.9354e-01, -5.8425e-02,  3.0704e-01, -8.3526e-02, -2.5457e-02,\n",
      "         -4.0698e-01, -2.1001e+00, -1.3526e-01, -1.7423e-01, -2.9494e-01,\n",
      "          1.6362e-02, -4.2271e-02, -2.0071e-03,  3.0743e-02, -2.7362e-01,\n",
      "          5.0478e-01, -2.2450e-01,  4.1450e-01,  3.2897e-01, -1.4867e-01,\n",
      "         -4.7380e-02, -1.5123e-01, -1.3995e-01,  3.7243e-01, -9.1023e-02,\n",
      "          1.6679e-01,  1.7505e-02,  4.3524e-01, -1.6702e-01,  3.7133e-01,\n",
      "         -3.0221e-01, -1.2800e-01,  3.9521e-01,  4.1832e-01, -5.4529e-02,\n",
      "         -3.8617e-01, -1.6286e-01, -4.7409e-02, -2.4131e-01, -6.3330e-02,\n",
      "          4.8083e-01,  3.4666e-01,  2.7711e-01,  4.8544e-01,  4.4631e-03,\n",
      "          3.8024e-01, -3.9694e-01, -7.4678e-02,  2.8513e-01,  9.7966e-01,\n",
      "         -1.9088e-01,  1.7128e-01, -6.3679e-02,  2.3878e-01,  2.1184e-01,\n",
      "          2.4592e-02, -1.6284e-01,  7.6438e-02, -3.9768e-03, -5.1758e-02,\n",
      "         -2.0785e-01,  3.8832e-01,  1.0074e-01, -2.6651e-01, -3.8993e-01,\n",
      "          2.8760e-01, -2.6789e-01, -3.5177e-01, -6.4066e-02, -2.9196e+00,\n",
      "         -1.2846e-01,  4.2029e-01, -3.7495e-01, -3.0970e-01,  1.7613e-01,\n",
      "          3.1321e-01,  4.9105e-01, -1.3627e-01,  9.8938e-03,  3.7281e-01,\n",
      "         -6.2187e-02,  1.4789e-01, -2.7523e-01, -3.0038e-01,  1.9778e-01,\n",
      "          4.7333e-01,  2.4103e-01, -2.7362e-01,  2.9643e-01,  7.9782e-02,\n",
      "         -7.5832e-02,  7.5407e-01,  9.2122e-02, -2.4376e-01, -2.6034e-01,\n",
      "          6.5333e-02,  1.9385e-01,  3.9409e-01, -5.7164e-02, -4.5007e-02,\n",
      "         -4.6776e-01, -4.7551e-01, -3.2879e+00,  1.5824e-01,  5.8268e-01,\n",
      "          2.9971e-02,  7.3100e-03,  1.4289e-01,  4.9981e-01,  3.1008e-01,\n",
      "          1.2842e-01, -1.3869e-01, -2.8301e-01,  3.2054e-01, -2.3203e-01,\n",
      "          7.2854e-02, -2.0410e-01,  7.6804e-02,  3.2959e-02,  5.8900e-01,\n",
      "         -9.1541e-04, -3.3636e-01, -1.3121e-01, -3.7715e-01, -1.3792e-01,\n",
      "          2.0697e-02,  1.8181e-01, -7.5197e-02, -2.6615e-02, -2.4267e-01,\n",
      "         -1.8429e-01, -5.9723e-01,  4.2487e-01,  9.3625e-02,  5.1002e-01,\n",
      "         -3.5778e-01,  2.1565e-01,  4.8708e-01,  5.7084e-02, -1.2604e-01,\n",
      "         -1.6871e-01,  4.6477e-01, -3.1817e-02, -4.8734e-02,  2.0707e-01,\n",
      "          1.0915e-01,  4.5673e-01, -2.9893e-01,  9.4988e-03, -1.3667e-01,\n",
      "         -3.2876e-01,  1.0954e-01,  4.6438e-01,  1.2322e-01,  3.0407e-01,\n",
      "          1.7988e-01,  6.5709e-02, -5.8161e-01,  3.0160e-01,  3.2965e-01,\n",
      "          1.1631e-01,  7.5220e-02, -9.8735e-02, -1.8966e-02, -3.8288e-01,\n",
      "          4.0549e+00,  4.4171e-01, -8.1849e-02, -1.2620e-02, -1.2036e-01,\n",
      "         -7.5879e-02,  9.6103e-02, -1.0706e-01, -1.1667e-01,  8.3064e-02,\n",
      "         -5.6658e-01,  6.4748e-01,  2.5069e-02, -1.7682e-01, -1.9978e-01,\n",
      "          5.7673e-01,  9.0092e-02,  9.7394e-02,  9.5260e-02, -3.4781e-01,\n",
      "          4.9521e-01, -1.0123e-01,  5.2690e-01, -1.4136e-01, -1.1896e+00,\n",
      "          8.9384e-03,  1.0771e-01, -4.3567e-01,  3.3649e-01, -4.3321e-01,\n",
      "          3.7671e-01,  9.4266e-03, -1.0057e-01, -9.0816e-03,  2.8219e-02,\n",
      "          3.7972e-01, -2.5953e-02,  2.2817e-01, -1.5621e-02, -4.4818e-01,\n",
      "          2.3306e-01,  7.2921e-01, -4.7979e-02,  1.7765e-01,  2.2178e-02,\n",
      "          5.8347e-01, -1.5730e-01,  1.2084e-01, -4.5277e-01, -1.5022e-01,\n",
      "         -1.0403e-01,  8.6694e-02, -4.6872e-02, -4.0580e-01, -1.6846e-01,\n",
      "         -3.7108e-01, -3.1635e-01,  2.4477e-01, -2.8952e-01, -1.0447e-02,\n",
      "         -4.2894e-01, -6.9695e-03, -3.3738e-01,  1.8687e-01, -1.9987e-01,\n",
      "         -6.5580e-02, -8.4402e-02,  7.1041e-02, -4.4556e+00, -7.1083e-02,\n",
      "          4.8723e-01,  2.2327e-01,  1.1680e-01, -2.4205e-01, -1.9197e-02,\n",
      "          5.5968e-01,  2.4977e-01, -4.3898e-01,  2.9982e-01,  9.4107e-02,\n",
      "          3.1934e-01,  1.8545e-02, -2.6918e-01,  4.7525e-01, -1.2339e-01,\n",
      "          1.1628e-01,  5.5468e-02, -2.4690e-01,  7.7857e-02,  1.9257e-01,\n",
      "         -3.9769e-01, -4.3133e-01,  3.4271e-01, -3.7285e-01, -1.6692e-01,\n",
      "         -1.4250e-01,  2.8041e-01, -4.1348e-01,  1.5749e-01,  1.3757e-01,\n",
      "          3.2011e-01, -2.1937e-02,  1.1167e-01, -2.4613e+00, -7.6562e-02,\n",
      "         -3.4483e-01, -3.9016e-02,  1.2307e-01, -3.7654e-01,  4.1617e-01,\n",
      "          5.0747e-02,  5.9004e-02, -6.7616e-03,  3.7792e-01,  1.2912e-01,\n",
      "          7.3294e-02,  2.8076e-01, -8.3379e-02,  2.9678e-01,  1.5440e-01,\n",
      "          3.4498e-01,  2.0933e-01, -2.3094e-01,  1.7974e-01, -2.4369e-02,\n",
      "          9.3670e-02, -1.9397e-01,  3.4693e-01,  2.4857e-01, -1.6463e-01,\n",
      "         -2.6402e-01, -3.3497e-01, -1.5281e-01, -1.1956e-02,  1.6691e-01,\n",
      "         -1.9148e-01,  1.3375e-01, -5.2295e-02, -2.6976e-01,  3.1546e-01,\n",
      "          2.0327e-01,  2.8132e-01,  3.8385e-02, -1.5460e-02,  4.8452e-01,\n",
      "         -1.3080e-01,  3.7337e-01,  4.3986e-02,  2.2948e-01, -9.9467e-02,\n",
      "         -5.9587e-02,  2.4163e-01, -3.7413e-02, -2.8505e-01, -1.0385e-01,\n",
      "          1.2844e+00,  3.7931e-02,  6.0662e-01, -1.3427e-01,  3.1566e-01,\n",
      "         -1.0915e-01,  3.3390e-01,  1.0486e-01,  7.7695e-01, -1.0951e-01,\n",
      "          3.3215e-01, -8.7887e-02, -5.3158e-02, -3.9932e-01,  1.6761e-01,\n",
      "         -1.3683e-01, -1.1485e-01, -7.1744e-02, -7.3496e-03,  3.5264e-01,\n",
      "          2.3055e-01, -5.6359e-01, -1.8131e-01,  2.3874e-01, -2.4617e-01,\n",
      "          2.7492e-01,  1.4022e-01, -2.8118e-01, -1.2551e-01, -5.3776e-02,\n",
      "          1.5835e-01,  4.5852e-01,  2.7455e-02, -4.9705e-02,  3.0009e-02,\n",
      "          8.3106e-02, -2.5613e-01,  2.7264e-01, -4.8304e-01,  2.2944e-01,\n",
      "          1.8664e-01,  1.3018e-01, -2.7470e-01,  1.2275e-01,  4.7687e-01,\n",
      "         -7.7422e-01,  6.8828e-02, -2.7489e-01, -4.4045e-01, -1.5664e-01,\n",
      "         -1.5963e-01,  3.1162e-01, -3.4174e-01, -9.1081e-02, -2.8513e-01,\n",
      "          1.4194e-01,  7.8740e-02,  8.7727e-02,  1.5238e-01,  1.6097e-01,\n",
      "         -1.2215e-01,  1.0461e-01,  8.6855e-01, -1.1700e-01, -1.5835e-01,\n",
      "          5.7599e-01,  4.1082e-01,  6.4968e-01,  2.4697e-01,  3.3254e-01,\n",
      "          6.0118e-02,  1.7962e-01, -2.0526e-01,  2.5433e-01,  6.5635e-01,\n",
      "         -1.1296e-01, -6.2403e-01, -3.5653e-01, -4.4770e-02, -1.8936e-01,\n",
      "          1.6203e-01, -7.3615e-01, -1.4689e-01,  1.0803e-02, -2.0177e-01,\n",
      "          5.1523e-01,  1.7079e-01,  3.4992e-02,  1.9688e-01, -7.5679e-03,\n",
      "         -1.5475e-01,  5.1012e-01, -8.8428e-02,  4.3343e-01,  2.8645e-01,\n",
      "         -8.2360e-02,  1.1619e-02,  7.8475e-01,  2.6095e-01, -4.4931e-01,\n",
      "          5.0914e-01,  2.5219e-02,  5.4225e-02, -2.8054e-01,  2.1993e-01,\n",
      "         -1.8856e-01, -5.2133e-02, -1.1354e-03,  3.1218e-03,  1.9515e-01,\n",
      "         -1.1181e+00,  3.1704e-01,  1.8378e-01, -2.5712e-01,  5.6485e-01,\n",
      "         -1.7888e-01, -3.5898e-01,  4.3856e-01,  1.2262e-01,  1.3512e-01,\n",
      "         -3.3540e-01, -2.5693e-01, -2.3704e-01,  2.8160e-01, -2.2310e-02,\n",
      "          2.6176e-02,  2.8206e-01, -6.7781e-02,  1.1555e-01,  1.9458e-01,\n",
      "         -1.0678e-01,  4.4277e-01, -5.2126e-03, -5.4186e-01,  5.7453e-02,\n",
      "          7.4920e-02, -6.1812e-02,  4.0058e-01,  6.7486e-02, -2.8046e-01,\n",
      "          2.5539e-01, -2.3814e-01, -2.9831e-01, -1.4721e-01, -3.4427e-02,\n",
      "          2.2456e-01,  1.6760e-01, -9.3027e-02,  6.2676e-01,  4.1513e-01,\n",
      "         -3.7543e-01, -4.4665e-03, -2.0270e-01, -2.5603e-01,  2.8555e-01,\n",
      "          2.9006e-01, -2.1721e-01,  3.9340e-01,  2.0000e-01, -2.8482e-01,\n",
      "          6.6162e-02,  2.8800e-01,  6.0611e-02, -1.0873e-01,  1.8840e-01,\n",
      "          1.1473e-01,  8.3191e-03, -2.5311e-01, -2.2236e-01, -3.9466e-01,\n",
      "          3.0283e-01, -9.5289e-02, -3.4977e-01,  2.6159e-01, -5.1095e-01,\n",
      "         -8.0120e-01,  1.1788e-01, -2.5433e-01, -2.6308e-01,  1.1189e-01,\n",
      "          3.4136e-01,  2.0907e-01, -1.2398e-02,  1.3119e-02, -5.3769e-01,\n",
      "          3.8781e-01, -2.8384e-01, -1.9673e-01,  1.2040e-01, -4.8356e-01,\n",
      "         -2.1569e-02, -6.4664e-01, -4.8668e-01,  2.0569e-01, -1.3885e-01,\n",
      "          7.5920e-02,  8.9313e-02,  5.4242e-02,  5.0753e-02, -4.1250e-03,\n",
      "         -2.0385e-01, -3.0445e-01,  6.7300e-01, -1.0125e-01,  1.0721e-01,\n",
      "         -2.5364e-01, -2.2689e-02, -8.6386e-02, -1.4693e-01,  1.2346e-01,\n",
      "          5.7882e-02,  3.8951e-01, -8.6798e-02,  4.3783e-01,  3.9253e-02,\n",
      "          3.6703e-01,  3.1195e-01,  1.7698e-01, -4.5262e-01,  1.6717e-01,\n",
      "          1.0562e-01,  1.9286e-01, -6.8559e-02, -2.7467e-01,  7.1649e-02,\n",
      "         -1.3638e-01, -1.6409e-01, -2.4427e-01,  2.2111e+00,  4.5720e-01,\n",
      "          3.0398e-01,  1.0331e-01,  5.7479e-02,  8.5001e-02, -3.6600e-01,\n",
      "         -4.8487e-02, -2.2746e-01,  1.6384e-01,  1.3247e-01, -1.4116e-01,\n",
      "         -4.1723e-01,  3.7333e-01, -4.5532e-02,  5.2252e-02, -4.8492e-01,\n",
      "         -5.7453e-01, -5.3271e-01, -1.2756e-02, -8.0918e-01,  5.7626e-01,\n",
      "          6.3891e-02, -1.1739e-01,  3.8096e-01, -1.2140e-01,  1.9883e-02,\n",
      "         -2.9249e-01, -1.5424e-01, -3.9374e-01,  5.2674e-03, -1.2914e-01,\n",
      "         -1.7073e-01,  1.3829e-01, -1.1900e-01,  2.2810e-02,  5.2490e-02,\n",
      "         -4.8994e-01, -1.8185e-01, -4.0703e-02,  1.0114e-01,  1.8774e-02,\n",
      "          4.9522e-01,  1.3091e-01,  1.7628e-01,  4.0179e-01, -1.5735e-01,\n",
      "         -2.3177e-01,  3.1240e-01,  6.2695e-02, -3.2510e-01, -4.6222e-01,\n",
      "         -5.4129e-01,  3.2981e-01, -5.7392e-01, -4.4484e-02,  1.1865e-01,\n",
      "         -9.3585e-02, -2.2068e-01,  5.6664e-01, -1.9118e-01,  4.1531e-01,\n",
      "          2.2360e-01, -4.4713e-01, -2.6913e-02, -7.3584e-02, -4.7656e-01,\n",
      "         -1.0550e-01, -1.2415e-01, -4.2647e-01,  7.9662e-02,  3.0926e-01,\n",
      "          2.6887e-01,  2.6424e-01,  2.0980e-01,  2.2612e-02,  1.7591e-01,\n",
      "         -1.7069e-01,  9.1757e-02, -3.2667e+00,  2.4992e-01, -2.5555e-01,\n",
      "          3.2626e-01,  6.1872e-02,  8.9982e-02, -1.9126e-01, -2.3481e-01,\n",
      "          2.3296e-01, -2.4742e-01,  1.3627e-01,  2.5100e-01,  1.7700e-01,\n",
      "          2.0548e-01, -2.8076e-02,  3.9666e-01,  2.6802e-01, -1.0573e-01,\n",
      "         -1.3370e-01,  5.5926e-02, -1.9257e-02, -3.4760e-01,  1.3862e-01,\n",
      "         -6.2903e-02, -5.0730e-01,  3.5035e-01, -3.0168e-01, -3.8748e-01,\n",
      "         -1.7461e-01,  1.9470e-01, -3.0012e-01,  3.5719e-01, -2.3690e-01,\n",
      "          2.9061e-01, -1.5717e-01, -1.9238e-01, -2.3596e-01,  1.5186e-02,\n",
      "          2.3970e-01,  9.6910e-02,  9.3351e-02,  1.9919e-01,  3.1349e-01,\n",
      "         -1.1629e-01, -3.2655e-01,  9.8714e-02, -1.2073e-02, -2.0701e-01,\n",
      "          6.6256e-01, -2.3074e-01,  1.2222e-01,  2.0928e-01, -1.6824e-01,\n",
      "          1.6958e-01,  6.6577e-01,  1.0463e-01, -2.2236e-02,  4.9558e-02,\n",
      "         -2.6131e-01, -2.6176e-02,  4.9758e-02, -1.4658e-01, -2.5339e-01,\n",
      "          2.0374e-01, -2.0308e-01, -2.1041e-01,  1.2410e-01, -2.3377e-02,\n",
      "         -1.3737e-01,  1.6805e-01, -1.9965e-01, -1.3039e-01,  3.2024e-01,\n",
      "         -1.2732e-01, -2.8263e-02,  3.7437e-01,  2.7242e-01,  9.9555e-02,\n",
      "          6.4697e-02, -5.0392e-02, -3.5171e-02, -4.3866e-01, -1.0558e-01,\n",
      "          2.3507e-01,  1.9894e-01, -8.2210e+00, -4.5828e-01, -6.2760e-01,\n",
      "         -2.7473e-01,  2.0936e-01, -1.0346e-01,  2.8234e-01,  2.1718e-01,\n",
      "          2.2534e-01,  2.6990e-01,  4.0459e-01, -2.0265e-01,  1.9840e-02,\n",
      "         -9.2059e-02,  1.9079e-01,  4.0046e-01]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get the output embeddings for the [CLS] token\n",
    "# which can be used as a fixed-dimensional representation of the input sentence\n",
    "cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "print(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_embedding.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an implementation where BERT tries to fill in the masked token.\n",
    "\n",
    "This is also a task on which the BERT model was trained. It was done using a technique called Masked Language Modelling (MLM). In this, a small percentage of the tokens passed during training BERT, were randomly masked (hidden), and it was BERT's task to fill them in, based on the context and sentence.  \n",
    "\n",
    "The B, standing for Bidirectional, means that the model looks for context in both directions.\n",
    "\n",
    "Another task that BERT was trained on, was Next Sentence Prediction (NSP). In this task, BERT is trained to predict whether a given pair of sentences are consecutive sentences in a document, or not.\n",
    "\n",
    "This also allowed the model to learn the relationship between different sentences in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! He was my cheeseburger.\n",
      "Hey! He ate my cheeseburger.\n",
      "Hey! He is my cheeseburger.\n",
      "Hey! He stole my cheeseburger.\n",
      "Hey! He saved my cheeseburger.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict = True)\n",
    "text = \"Hey! He \" + tokenizer.mask_token + \" my cheeseburger.\"\n",
    "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "logits = model(**input)\n",
    "logits = logits.logits\n",
    "softmax = F.softmax(logits, dim = -1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_5 = torch.topk(mask_word, 5, dim = 1)[1][0]\n",
    "for token in top_5:\n",
    "   word = tokenizer.decode([token])\n",
    "   new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "   print(new_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
