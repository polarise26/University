{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5P23Kl-GolV",
        "outputId": "e156b637-aa75-4395-d8fb-84b5a0db6801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: braceexpand in /usr/local/lib/python3.8/dist-packages (0.1.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install braceexpand"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXUVuHSEG3LH",
        "outputId": "68401d1b-57bb-43aa-f032-41e74fdea828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.8/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.8/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.8/dist-packages (from omegaconf) (6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGTST0WFG6nx",
        "outputId": "292d907a-36e8-4262-f21f-fcda976d980c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from torch_optimizer) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->torch_optimizer) (4.4.0)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg6Jw4WjMXci",
        "outputId": "e6697aac-1fa4-47ab-dd0b-0daac35d6153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clip\n",
            "  Downloading clip-0.2.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=7005 sha256=d624da51463a570b4dfe1dda820416e27bb4fbf2b4ef06fbeaf349eb34a52101\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/17/18/6193c6b02f9e35e3b3f0721a349b9f9f74bac11feb0f86fdd1\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/pvigier/perlin-numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FjyfRhAQ6xM",
        "outputId": "1e069bd5-21bd-4e10-c19b-a0f18d761e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/pvigier/perlin-numpy\n",
            "  Cloning https://github.com/pvigier/perlin-numpy to /tmp/pip-req-build-60ygp3t_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pvigier/perlin-numpy /tmp/pip-req-build-60ygp3t_\n",
            "  Resolved https://github.com/pvigier/perlin-numpy to commit 5e26837db14042e51166eb6cad4c0df2c1907016\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from perlin-numpy==0.0.0) (1.21.6)\n",
            "Building wheels for collected packages: perlin-numpy\n",
            "  Building wheel for perlin-numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for perlin-numpy: filename=perlin_numpy-0.0.0-py3-none-any.whl size=4752 sha256=e87682ec1b3e602cc5c14ef0db2cf060ab116ef05621413bf42cec27a6c4f06e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y9xxdeow/wheels/42/f1/aa/a01cebc65547819ce7e859ed78b46258e145fd7cdc29147efd\n",
            "Successfully built perlin-numpy\n",
            "Installing collected packages: perlin-numpy\n",
            "Successfully installed perlin-numpy-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"clipit\")\n",
        "import argparse\n",
        "import math\n",
        "from urllib.request import urlopen\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import glob\n",
        "from braceexpand import braceexpand\n",
        "from types import SimpleNamespace\n",
        "import os.path\n",
        "from omegaconf import OmegaConf"
      ],
      "metadata": {
        "id": "oZZP22BTG-av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "torch.backends.cudnn.benchmark = False\t\t# NR: True is a bit faster, but can lead to OOM. False is more deterministic.\n",
        "#torch.use_deterministic_algorithms(True)\t\t# NR: grid_sampler_2d_backward_cuda does not have a deterministic implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "V9JGxlAYHfra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_optimizer import DiffGrad, AdamP, RAdam\n",
        "from perlin_numpy import generate_fractal_noise_2d\n"
      ],
      "metadata": {
        "id": "57Xq091gHuwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vqgan-clp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMtflJXxVmcm",
        "outputId": "9d9521ca-4cd2-4141-b245-755277444a73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement vqgan-clp (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for vqgan-clp\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFile, Image, PngImagePlugin\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# or 'border'\n",
        "global_padding_mode = 'reflection'\n",
        "global_aspect_width = 1\n",
        "global_spot_file = None\n",
        "\n",
        "from vqgan import VqganDrawer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "VuSD_235IIy7",
        "outputId": "0dded445-dbb9-471c-8f22-8ac9ef3b971a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-3d6a777bbd41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mglobal_spot_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvqgan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVqganDrawer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vqgan'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from clipdrawer import ClipDrawer\n",
        "except ImportError:\n",
        "    pass\n",
        "    # print('clipdrawer not imported')\n",
        "try:\n",
        "    from pixeldrawer import PixelDrawer\n",
        "except ImportError:\n",
        "    pass\n",
        "    # print('pixeldrawer not imported')\n",
        "\n",
        "try:\n",
        "    import matplotlib.colors\n",
        "except ImportError:\n",
        "    # only needed for palette stuff\n",
        "    pass\n",
        "\n",
        "# print(\"warning: running unreleased future version\")\n"
      ],
      "metadata": {
        "id": "_Zb4NKLAIdp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/a/39662359\n",
        "def isnotebook():\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif shell == 'Shell':\n",
        "            return True   # Seems to be what co-lab does\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other type (?)\n",
        "    except NameError:\n",
        "        return False      # Probably standard Python interpreter\n",
        "\n",
        "IS_NOTEBOOK = isnotebook()\n",
        "\n",
        "if IS_NOTEBOOK:\n",
        "    from IPython import display\n",
        "    from tqdm.notebook import tqdm\n",
        "    from IPython.display import clear_output\n",
        "else:\n",
        "    from tqdm import tqdm"
      ],
      "metadata": {
        "id": "-5KaliP5IlpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file helpers\n",
        "def real_glob(rglob):\n",
        "    glob_list = braceexpand(rglob)\n",
        "    files = []\n",
        "    for g in glob_list:\n",
        "        files = files + glob.glob(g)\n",
        "    return sorted(files)\n"
      ],
      "metadata": {
        "id": "Atwbc9mlIxWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions and classes\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n"
      ],
      "metadata": {
        "id": "F_YfwFQII22J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()"
      ],
      "metadata": {
        "id": "PYiGyVHII7fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]"
      ],
      "metadata": {
        "id": "XyD9wbCjJE0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NR: Testing with different intital images\n",
        "def old_random_noise_image(w,h):\n",
        "    random_image = Image.fromarray(np.random.randint(0,255,(w,h,3),dtype=np.dtype('uint8')))\n",
        "    return random_image\n"
      ],
      "metadata": {
        "id": "XTNzqrjyJJH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NormalizeData(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
      ],
      "metadata": {
        "id": "w00KW13IJQ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stats.stackexchange.com/a/289477\n",
        "def contrast_noise(n):\n",
        "    n = 0.9998 * n + 0.0001\n",
        "    n1 = (n / (1-n))\n",
        "    n2 = np.power(n1, -2)\n",
        "    n3 = 1 / (1 + n2)\n",
        "    return n3"
      ],
      "metadata": {
        "id": "OU7VRfTRJc0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_noise_image(w,h):\n",
        "    # scale up roughly as power of 2\n",
        "    if (w>1024 or h>1024):\n",
        "        side, octp = 2048, 7\n",
        "    elif (w>512 or h>512):\n",
        "        side, octp = 1024, 6\n",
        "    elif (w>256 or h>256):\n",
        "        side, octp = 512, 5\n",
        "    else:\n",
        "        side, octp = 256, 4\n",
        "\n",
        "    nr = NormalizeData(generate_fractal_noise_2d((side, side), (32, 32), octp))\n",
        "    ng = NormalizeData(generate_fractal_noise_2d((side, side), (32, 32), octp))\n",
        "    nb = NormalizeData(generate_fractal_noise_2d((side, side), (32, 32), octp))\n",
        "    stack = np.dstack((contrast_noise(nr),contrast_noise(ng),contrast_noise(nb)))\n",
        "    substack = stack[:h, :w, :]\n",
        "    im = Image.fromarray((255.9 * stack).astype('uint8'))\n",
        "    return im"
      ],
      "metadata": {
        "id": "q5BZQvl3Jh85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "def gradient_2d(start, stop, width, height, is_horizontal):\n",
        "    if is_horizontal:\n",
        "        return np.tile(np.linspace(start, stop, width), (height, 1))\n",
        "    else:\n",
        "        return np.tile(np.linspace(start, stop, height), (width, 1)).T\n"
      ],
      "metadata": {
        "id": "4BO5u6_JJnSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_3d(width, height, start_list, stop_list, is_horizontal_list):\n",
        "    result = np.zeros((height, width, len(start_list)), dtype=float)\n",
        "\n",
        "    for i, (start, stop, is_horizontal) in enumerate(zip(start_list, stop_list, is_horizontal_list)):\n",
        "        result[:, :, i] = gradient_2d(start, stop, width, height, is_horizontal)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "QWPAACU0Jsvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_gradient_image(w,h):\n",
        "    array = gradient_3d(w, h, (0, 0, np.random.randint(0,255)), (np.random.randint(1,255), np.random.randint(2,255), np.random.randint(3,128)), (True, False, False))\n",
        "    random_image = Image.fromarray(np.uint8(array))\n",
        "    return random_image\n"
      ],
      "metadata": {
        "id": "X-ajdeagJyju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "replace_grad = ReplaceGrad.apply"
      ],
      "metadata": {
        "id": "Z0V_XYQ1J2W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n"
      ],
      "metadata": {
        "id": "Cw1PT4-oJ8Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()"
      ],
      "metadata": {
        "id": "5NXYPXU6KA0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    # print(f\"parsed vals is {vals}\")\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "\n",
        "from typing import cast, Dict, List, Optional, Tuple, Union\n"
      ],
      "metadata": {
        "id": "Oa3x1vceKNk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# override class to get padding_mode\n",
        "class MyRandomPerspective(K.RandomPerspective):\n",
        "    def apply_transform(\n",
        "        self, input: torch.Tensor, params: Dict[str, torch.Tensor], transform: Optional[torch.Tensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        _, _, height, width = input.shape\n",
        "        transform = cast(torch.Tensor, transform)\n",
        "        return kornia.geometry.warp_perspective(\n",
        "            input, transform, (height, width),\n",
        "             mode=self.resample.name.lower(), align_corners=self.align_corners, padding_mode=global_padding_mode\n",
        "        )\n",
        "\n",
        "cached_spot_indexes = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "E5iJnShWKS9v",
        "outputId": "370a7f46-1294-4b87-f180-9ced735a5b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-946fe8a6a7f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# override class to get padding_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyRandomPerspective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomPerspective\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     def apply_transform(\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ) -> torch.Tensor:\n",
            "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_spot_indexes(sideX, sideY):\n",
        "    global global_spot_file\n",
        "\n",
        "    # make sure image is loaded if we need it\n",
        "    cache_key = (sideX, sideY)\n",
        "\n",
        "    if cache_key not in cached_spot_indexes:\n",
        "        if global_spot_file is not None:\n",
        "            mask_image = Image.open(global_spot_file)\n",
        "        elif global_aspect_width != 1:\n",
        "            mask_image = Image.open(\"inputs/spot_wide.png\")\n",
        "        else:\n",
        "            mask_image = Image.open(\"inputs/spot_square.png\")\n",
        "        # this is a one channel mask\n",
        "        mask_image = mask_image.convert('RGB')\n",
        "        mask_image = mask_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "        mask_image_tensor = TF.to_tensor(mask_image)\n",
        "        # print(\"ONE CHANNEL \", mask_image_tensor.shape)\n",
        "        mask_indexes = mask_image_tensor.ge(0.5).to(device)\n",
        "        # print(\"GE \", mask_indexes.shape)\n",
        "        # sys.exit(0)\n",
        "        mask_indexes_off = mask_image_tensor.lt(0.5).to(device)\n",
        "        cached_spot_indexes[cache_key] = [mask_indexes, mask_indexes_off]\n",
        "\n",
        "    return cached_spot_indexes[cache_key]\n"
      ],
      "metadata": {
        "id": "zmiFL_42KeUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n = torch.ones((3,5,5))\n",
        "# f = generate.fetch_spot_indexes(5, 5)\n",
        "# f[0].shape = [60,3]\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        global global_aspect_width\n",
        "\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cutn_zoom = int(2*cutn/3)\n",
        "        self.cut_pow = cut_pow\n",
        "        self.transforms = None\n",
        "\n",
        "        augmentations = []\n",
        "        if global_aspect_width != 1:\n",
        "            augmentations.append(K.RandomCrop(size=(self.cut_size,self.cut_size), p=1.0, cropping_mode=\"resample\", return_transform=True))\n",
        "        augmentations.append(MyRandomPerspective(distortion_scale=0.40, p=0.7, return_transform=True))\n",
        "        augmentations.append(K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,0.75),  ratio=(0.85,1.2), cropping_mode='resample', p=0.7, return_transform=True))\n",
        "        augmentations.append(K.ColorJitter(hue=0.1, saturation=0.1, p=0.8, return_transform=True))\n",
        "        self.augs_zoom = nn.Sequential(*augmentations)\n",
        "\n",
        "        augmentations = []\n",
        "        if global_aspect_width == 1:\n",
        "            n_s = 0.95\n",
        "            n_t = (1-n_s)/2\n",
        "            augmentations.append(K.RandomAffine(degrees=0, translate=(n_t, n_t), scale=(n_s, n_s), p=1.0, return_transform=True))\n",
        "        elif global_aspect_width > 1:\n",
        "            n_s = 1/global_aspect_width\n",
        "            n_t = (1-n_s)/2\n",
        "            augmentations.append(K.RandomAffine(degrees=0, translate=(0, n_t), scale=(0.9*n_s, n_s), p=1.0, return_transform=True))\n",
        "        else:\n",
        "            n_s = global_aspect_width\n",
        "            n_t = (1-n_s)/2\n",
        "            augmentations.append(K.RandomAffine(degrees=0, translate=(n_t, 0), scale=(0.9*n_s, n_s), p=1.0, return_transform=True))\n",
        "\n",
        "\n",
        "             # augmentations.append(K.CenterCrop(size=(self.cut_size,self.cut_size), p=1.0, cropping_mode=\"resample\", return_transform=True))\n",
        "        augmentations.append(K.CenterCrop(size=self.cut_size, cropping_mode='resample', p=1.0, return_transform=True))\n",
        "        augmentations.append(K.RandomPerspective(distortion_scale=0.20, p=0.7, return_transform=True))\n",
        "        augmentations.append(K.ColorJitter(hue=0.1, saturation=0.1, p=0.8, return_transform=True))\n",
        "        self.augs_wide = nn.Sequential(*augmentations)\n",
        "\n",
        "        self.noise_fac = 0.1\n",
        "        \n",
        "        # Pooling\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n"
      ],
      "metadata": {
        "id": "HHLycFbWKsgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, input, spot=None):\n",
        "        global global_aspect_width, cur_iteration\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        mask_indexes = None\n",
        "\n",
        "        if spot is not None:\n",
        "            spot_indexes = fetch_spot_indexes(self.cut_size, self.cut_size)\n",
        "            if spot == 0:\n",
        "                mask_indexes = spot_indexes[1]\n",
        "            else:\n",
        "                mask_indexes = spot_indexes[0]\n",
        "            # print(\"Mask indexes \", mask_indexes)\n",
        "\n",
        "        for _ in range(self.cutn):\n",
        "            # Pooling\n",
        "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
        "\n",
        "            if mask_indexes is not None:\n",
        "                cutout[0][mask_indexes] = 0.5\n",
        "\n",
        "            if global_aspect_width != 1:\n",
        "                if global_aspect_width > 1:\n",
        "                    cutout = kornia.geometry.transform.rescale(cutout, (1, global_aspect_width))\n",
        "                else:\n",
        "                    cutout = kornia.geometry.transform.rescale(cutout, (1/global_aspect_width, 1))\n",
        "\n",
        "            # if cur_iteration % 50 == 0 and _ == 0:\n",
        "            #     print(cutout.shape)\n",
        "            #     TF.to_pil_image(cutout[0].cpu()).save(f\"cutout_im_{cur_iteration:02d}_{spot}.png\")\n",
        "\n",
        "            cutouts.append(cutout)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            # print(\"Cached transforms available\")\n",
        "            batch1 = kornia.geometry.transform.warp_perspective(torch.cat(cutouts[:self.cutn_zoom], dim=0), self.transforms[:self.cutn_zoom],\n",
        "                (self.cut_size, self.cut_size), padding_mode=global_padding_mode)\n",
        "            batch2 = kornia.geometry.transform.warp_perspective(torch.cat(cutouts[self.cutn_zoom:], dim=0), self.transforms[self.cutn_zoom:],\n",
        "                (self.cut_size, self.cut_size), padding_mode='zeros')\n",
        "            batch = torch.cat([batch1, batch2])\n",
        "            # if cur_iteration < 2:\n",
        "            #     for j in range(4):\n",
        "            #         TF.to_pil_image(batch[j].cpu()).save(f\"cached_im_{cur_iteration:02d}_{j:02d}_{spot}.png\")\n",
        "            #         j_wide = j + self.cutn_zoom\n",
        "            #         TF.to_pil_image(batch[j_wide].cpu()).save(f\"cached_im_{cur_iteration:02d}_{j_wide:02d}_{spot}.png\")\n",
        "        else:\n",
        "            batch1, transforms1 = self.augs_zoom(torch.cat(cutouts[:self.cutn_zoom], dim=0))\n",
        "            batch2, transforms2 = self.augs_wide(torch.cat(cutouts[self.cutn_zoom:], dim=0))\n",
        "            # print(batch1.shape, batch2.shape)\n",
        "            batch = torch.cat([batch1, batch2])\n",
        "            # print(batch.shape)\n",
        "            self.transforms = torch.cat([transforms1, transforms2])\n",
        "            ## batch, self.transforms = self.augs(torch.cat(cutouts, dim=0))\n",
        "            # if cur_iteration < 2:\n",
        "            #     for j in range(4):\n",
        "            #         TF.to_pil_image(batch[j].cpu()).save(f\"live_im_{cur_iteration:02d}_{j:02d}_{spot}.png\")\n",
        "            #         j_wide = j + self.cutn_zoom\n",
        "            #         TF.to_pil_image(batch[j_wide].cpu()).save(f\"live_im_{cur_iteration:02d}_{j_wide:02d}_{spot}.png\")\n",
        "\n",
        "        # print(batch.shape, self.transforms.shape)\n",
        "        \n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n"
      ],
      "metadata": {
        "id": "15toMRSkK1qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n"
      ],
      "metadata": {
        "id": "IXEwHSDALS8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_init(args):\n",
        "    global opts, perceptors, normalize, cutoutsTable, cutoutSizeTable\n",
        "    global z_orig, z_targets, z_labels, init_image_tensor, target_image_tensor\n",
        "    global gside_X, gside_Y, overlay_image_rgba\n",
        "    global pmsTable, pmsImageTable, pImages, device, spotPmsTable, spotOffPmsTable\n",
        "    global drawer\n",
        "\n",
        "    # Do it (init that is)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if args.use_clipdraw:\n",
        "        drawer = ClipDrawer(args.size[0], args.size[1], args.strokes)\n",
        "    elif args.use_pixeldraw:\n",
        "        if args.pixel_size is not None:\n",
        "            drawer = PixelDrawer(args.size[0], args.size[1], args.do_mono, args.pixel_size, scale=args.pixel_scale)\n",
        "        elif global_aspect_width == 1:\n",
        "            drawer = PixelDrawer(args.size[0], args.size[1], args.do_mono, [40, 40], scale=args.pixel_scale)\n",
        "        else:\n",
        "            drawer = PixelDrawer(args.size[0], args.size[1], args.do_mono, scale=args.pixel_scale)\n",
        "    else:\n",
        "        drawer = VqganDrawer(args.vqgan_model)\n",
        "    drawer.load_model(args.vqgan_config, args.vqgan_checkpoint, device)\n",
        "    num_resolutions = drawer.get_num_resolutions()\n",
        "    # print(\"-----------> NUMR \", num_resolutions)\n",
        "\n",
        "    jit = True if float(torch.__version__[:3]) < 1.8 else False\n",
        "    f = 2**(num_resolutions - 1)\n",
        "\n",
        "    toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "    sideX, sideY = toksX * f, toksY * f\n",
        "\n",
        "    # save sideX, sideY in globals (need if using overlay)\n",
        "    gside_X = sideX\n",
        "    gside_Y = sideY\n",
        "\n",
        "    for clip_model in args.clip_models:\n",
        "        perceptor = clip.load(clip_model, jit=jit)[0].eval().requires_grad_(False).to(device)\n",
        "        perceptors[clip_model] = perceptor\n",
        "\n",
        "        cut_size = perceptor.visual.input_resolution\n",
        "        cutoutSizeTable[clip_model] = cut_size\n",
        "        if not cut_size in cutoutsTable:    \n",
        "            make_cutouts = MakeCutouts(cut_size, args.num_cuts, cut_pow=args.cut_pow)\n",
        "            cutoutsTable[cut_size] = make_cutouts\n",
        "\n",
        "    init_image_tensor = None\n",
        "    target_image_tensor = None\n",
        "\n",
        "    # Image initialisation\n",
        "    if args.init_image or args.init_noise:\n",
        "        # setup init image wih pil\n",
        "        # first - always start with noise or blank\n",
        "        if args.init_noise == 'pixels':\n",
        "            img = random_noise_image(args.size[0], args.size[1])\n",
        "        elif args.init_noise == 'gradient':\n",
        "            img = random_gradient_image(args.size[0], args.size[1])\n",
        "        elif args.init_noise == 'snow':\n",
        "            img = old_random_noise_image(args.size[0], args.size[1])\n",
        "        else:\n",
        "            img = Image.new(mode=\"RGB\", size=(args.size[0], args.size[1]), color=(255, 255, 255))\n",
        "        starting_image = img.convert('RGB')\n",
        "        starting_image = starting_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "\n",
        "        if args.init_image:\n",
        "            # now we might overlay an init image (init_image also can be recycled as overlay)\n",
        "            if 'http' in args.init_image:\n",
        "              init_image = Image.open(urlopen(args.init_image))\n",
        "            else:\n",
        "              init_image = Image.open(args.init_image)\n",
        "            # this version is needed potentially for the loss function\n",
        "            init_image_rgb = init_image.convert('RGB')\n",
        "            init_image_rgb = init_image_rgb.resize((sideX, sideY), Image.LANCZOS)\n",
        "            init_image_tensor = TF.to_tensor(init_image_rgb)\n",
        "            init_image_tensor = init_image_tensor.to(device).unsqueeze(0)\n",
        "\n",
        "            # this version gets overlaid on the background (noise)\n",
        "            init_image_rgba = init_image.convert('RGBA')\n",
        "            init_image_rgba = init_image_rgba.resize((sideX, sideY), Image.LANCZOS)\n",
        "            top_image = init_image_rgba.copy()\n",
        "            if args.init_image_alpha and args.init_image_alpha >= 0:\n",
        "                top_image.putalpha(args.init_image_alpha)\n",
        "            starting_image.paste(top_image, (0, 0), top_image)\n",
        "\n",
        "        starting_image.save(\"starting_image.png\")\n",
        "        starting_tensor = TF.to_tensor(starting_image)\n",
        "        init_tensor = starting_tensor.to(device).unsqueeze(0) * 2 - 1\n",
        "        drawer.init_from_tensor(init_tensor)\n",
        "\n",
        "    else:\n",
        "        # untested\n",
        "        drawer.rand_init(toksX, toksY)\n",
        "\n",
        "    if args.overlay_every:\n",
        "        if args.overlay_image:\n",
        "            if 'http' in args.overlay_image:\n",
        "              overlay_image = Image.open(urlopen(args.overlay_image))\n",
        "            else:\n",
        "              overlay_image = Image.open(args.overlay_image)\n",
        "            overlay_image_rgba = overlay_image.convert('RGBA')\n",
        "            overlay_image_rgba = overlay_image_rgba.resize((sideX, sideY), Image.LANCZOS)\n",
        "        else:\n",
        "            overlay_image_rgba = init_image_rgba\n",
        "        if args.overlay_alpha:\n",
        "            overlay_image_rgba.putalpha(args.overlay_alpha)\n",
        "        overlay_image_rgba.save('overlay_image.png')\n",
        "\n",
        "    if args.target_images is not None:\n",
        "        z_targets = []\n",
        "        filelist = real_glob(args.target_images)\n",
        "        for target_image in filelist:\n",
        "            target_image = Image.open(target_image)\n",
        "            target_image_rgb = target_image.convert('RGB')\n",
        "            target_image_rgb = target_image_rgb.resize((sideX, sideY), Image.LANCZOS)\n",
        "            target_image_tensor_local = TF.to_tensor(target_image_rgb)\n",
        "            target_image_tensor = target_image_tensor_local.to(device).unsqueeze(0) * 2 - 1\n",
        "            z_target = drawer.get_z_from_tensor(target_image_tensor)\n",
        "            z_targets.append(z_target)\n",
        "\n",
        "    if args.image_labels is not None:\n",
        "        z_labels = []\n",
        "        filelist = real_glob(args.image_labels)\n",
        "        cur_labels = []\n",
        "        for image_label in filelist:\n",
        "            image_label = Image.open(image_label)\n",
        "            image_label_rgb = image_label.convert('RGB')\n",
        "            image_label_rgb = image_label_rgb.resize((sideX, sideY), Image.LANCZOS)\n",
        "            image_label_rgb_tensor = TF.to_tensor(image_label_rgb)\n",
        "            image_label_rgb_tensor = image_label_rgb_tensor.to(device).unsqueeze(0) * 2 - 1\n",
        "            z_label = drawer.get_z_from_tensor(image_label_rgb_tensor)\n",
        "            cur_labels.append(z_label)\n",
        "        image_embeddings = torch.stack(cur_labels)\n",
        "        print(\"Processing labels: \", image_embeddings.shape)\n",
        "        image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n",
        "        image_embeddings = image_embeddings.mean(dim=0)\n",
        "        image_embeddings /= image_embeddings.norm()\n",
        "        z_labels.append(image_embeddings.unsqueeze(0))\n",
        "\n",
        "    z_orig = drawer.get_z_copy()\n",
        "\n",
        "    pmsTable = {}\n",
        "    pmsImageTable = {}\n",
        "    spotPmsTable = {}\n",
        "    spotOffPmsTable = {}\n",
        "    for clip_model in args.clip_models:\n",
        "        pmsTable[clip_model] = []\n",
        "        pmsImageTable[clip_model] = []\n",
        "        spotPmsTable[clip_model] = []\n",
        "        spotOffPmsTable[clip_model] = []\n",
        "    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                      std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "    # CLIP tokenize/encode\n",
        "    # NR: Weights / blending\n",
        "    for prompt in args.prompts:\n",
        "        for clip_model in args.clip_models:\n",
        "            pMs = pmsTable[clip_model]\n",
        "            perceptor = perceptors[clip_model]\n",
        "            txt, weight, stop = parse_prompt(prompt)\n",
        "            embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "    for prompt in args.spot_prompts:\n",
        "        for clip_model in args.clip_models:\n",
        "            pMs = spotPmsTable[clip_model]\n",
        "            perceptor = perceptors[clip_model]\n",
        "            txt, weight, stop = parse_prompt(prompt)\n",
        "            embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "    for prompt in args.spot_prompts_off:\n",
        "        for clip_model in args.clip_models:\n",
        "            pMs = spotOffPmsTable[clip_model]\n",
        "            perceptor = perceptors[clip_model]\n",
        "            txt, weight, stop = parse_prompt(prompt)\n",
        "            embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "            pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "    for label in args.labels:\n",
        "        for clip_model in args.clip_models:\n",
        "            pMs = pmsTable[clip_model]\n",
        "            perceptor = perceptors[clip_model]\n",
        "            txt, weight, stop = parse_prompt(label)\n",
        "            texts = [template.format(txt) for template in imagenet_templates] #format with class\n",
        "            print(f\"Tokenizing all of {texts}\")\n",
        "            texts = clip.tokenize(texts).to(device) #tokenize\n",
        "            class_embeddings = perceptor.encode_text(texts) #embed with text encoder\n",
        "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "            class_embedding = class_embeddings.mean(dim=0)\n",
        "            class_embedding /= class_embedding.norm()\n",
        "            pMs.append(Prompt(class_embedding.unsqueeze(0), weight, stop).to(device))\n",
        "\n",
        "    for clip_model in args.clip_models:\n",
        "        pImages = pmsImageTable[clip_model]\n",
        "        for prompt in args.image_prompts:\n",
        "            path, weight, stop = parse_prompt(prompt)\n",
        "            img = Image.open(path)\n",
        "            pil_image = img.convert('RGB')\n",
        "            img = resize_image(pil_image, (sideX, sideY))\n",
        "            pImages.append(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "\n",
        "    for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "        gen = torch.Generator().manual_seed(seed)\n",
        "        embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "        pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "    opts = drawer.get_opts()\n",
        "    if opts == None:\n",
        "        # legacy\n",
        "\n",
        "        # Set the optimiser\n",
        "        z = drawer.get_z();\n",
        "        if args.optimiser == \"Adam\":\n",
        "            opt = optim.Adam([z], lr=args.learning_rate)\t\t# LR=0.1\n",
        "        elif args.optimiser == \"AdamW\":\n",
        "            opt = optim.AdamW([z], lr=args.learning_rate)\t\t# LR=0.2\n",
        "        elif args.optimiser == \"Adagrad\":\n",
        "            opt = optim.Adagrad([z], lr=args.learning_rate)\t# LR=0.5+\n",
        "        elif args.optimiser == \"Adamax\":\n",
        "            opt = optim.Adamax([z], lr=args.learning_rate)\t# LR=0.5+?\n",
        "        elif args.optimiser == \"DiffGrad\":\n",
        "            opt = DiffGrad([z], lr=args.learning_rate)\t\t# LR=2+?\n",
        "        elif args.optimiser == \"AdamP\":\n",
        "            opt = AdamP([z], lr=args.learning_rate)\t\t# LR=2+?\n",
        "        elif args.optimiser == \"RAdam\":\n",
        "            opt = RAdam([z], lr=args.learning_rate)\t\t# LR=2+?\n",
        "\n",
        "        opts = [opt]\n",
        "\n",
        "    # Output for the user\n",
        "    print('Using device:', device)\n",
        "    print('Optimising using:', args.optimiser)\n",
        "\n",
        "    if args.prompts:\n",
        "        print('Using text prompts:', args.prompts)\n",
        "    if args.spot_prompts:\n",
        "        print('Using spot prompts:', args.spot_prompts)\n",
        "    if args.spot_prompts_off:\n",
        "        print('Using spot off prompts:', args.spot_prompts_off)\n",
        "    if args.image_prompts:\n",
        "        print('Using image prompts:', args.image_prompts)\n",
        "    if args.init_image:\n",
        "        print('Using initial image:', args.init_image)\n",
        "    if args.noise_prompt_weights:\n",
        "        print('Noise prompt weights:', args.noise_prompt_weights)\n",
        "\n",
        "\n",
        "    if args.seed is None:\n",
        "        seed = torch.seed()\n",
        "    else:\n",
        "        seed = args.seed\n",
        "    torch.manual_seed(seed)\n",
        "    print('Using seed:', seed)\n",
        "\n",
        "\n",
        "# dreaded globals (for now)\n",
        "z_orig = None\n",
        "z_targets = None\n",
        "z_labels = None\n",
        "opts = None\n",
        "drawer = None\n",
        "perceptors = {}\n",
        "normalize = None\n",
        "cutoutsTable = {}\n",
        "cutoutSizeTable = {}\n",
        "init_image_tensor = None\n",
        "target_image_tensor = None\n",
        "pmsTable = None\n",
        "spotPmsTable = None \n",
        "spotOffPmsTable = None \n",
        "pmsImageTable = None\n",
        "gside_X=None\n",
        "gside_Y=None\n",
        "overlay_image_rgba=None\n",
        "device=None\n",
        "cur_iteration=None\n",
        "cur_anim_index=None\n",
        "anim_output_files=[]\n",
        "anim_cur_zs=[]\n",
        "anim_next_zs=[]"
      ],
      "metadata": {
        "id": "VjjCKJD_LY9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gif(args, iter):\n",
        "    gif_output = os.path.join(args.animation_dir, \"anim.gif\")\n",
        "    if os.path.exists(gif_output):\n",
        "        os.remove(gif_output)\n",
        "    cmd = ['ffmpeg', '-framerate', '10', '-pattern_type', 'glob',\n",
        "           '-i', f\"{args.animation_dir}/*.png\", '-loop', '0', gif_output]\n",
        "    try:\n",
        "        output = subprocess.check_output(cmd)\n",
        "    except subprocess.CalledProcessError as cpe:\n",
        "        output = cpe.output\n",
        "        print(\"Ignoring non-zero exit: \", output)\n",
        "\n",
        "    return gif_output\n",
        "\n",
        "# !ffmpeg \\\n",
        "#   -framerate 10 -pattern_type glob \\\n",
        "#   -i '{animation_output}/*_*.png' \\\n",
        "#   -loop 0 {animation_output}/final.gif\n",
        "\n",
        " @torch.no_grad()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "qhOMgXMrL4s8",
        "outputId": "2e8c0492-4319-4614-e21b-35a5ed05b490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6233b07b6398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#   -loop 0 {animation_output}/final.gif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def checkin(args, iter, losses):\n",
        "    global drawer\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    writestr = f'iter: {iter}, loss: {sum(losses).item():g}, losses: {losses_str}'\n",
        "    if args.animation_dir is not None:\n",
        "        writestr = f'anim: {cur_anim_index}/{len(anim_output_files)} {writestr}'\n",
        "    tqdm.write(writestr)\n",
        "    info = PngImagePlugin.PngInfo()\n",
        "    info.add_text('comment', f'{args.prompts}')\n",
        "    img = drawer.to_image()\n",
        "    if cur_anim_index is None:\n",
        "        outfile = args.output\n",
        "    else:\n",
        "        outfile = anim_output_files[cur_anim_index]\n",
        "    img.save(outfile, pnginfo=info)\n",
        "    if cur_anim_index == len(anim_output_files) - 1:\n",
        "        # save gif\n",
        "        gif_output = make_gif(args, iter)\n",
        "        if IS_NOTEBOOK and iter % args.display_every == 0:\n",
        "            clear_output()\n",
        "            display.display(display.Image(open(gif_output,'rb').read()))\n",
        "    if IS_NOTEBOOK and iter % args.display_every == 0:\n",
        "        if cur_anim_index is None or iter == 0:\n",
        "            display.display(display.Image(outfile))\n"
      ],
      "metadata": {
        "id": "Q7zWFRMbML4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ascend_txt(args):\n",
        "    global cur_iteration, cur_anim_index, perceptors, normalize, cutoutsTable, cutoutSizeTable\n",
        "    global z_orig, z_targets, z_labels, init_image_tensor, target_image_tensor, drawer\n",
        "    global pmsTable, pmsImageTable, spotPmsTable, spotOffPmsTable, global_padding_mode\n",
        "\n",
        "    out = drawer.synth(cur_iteration);\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if (cur_iteration%2 == 0):\n",
        "        global_padding_mode = 'reflection'\n",
        "    else:\n",
        "        global_padding_mode = 'border'\n",
        "\n",
        "    cur_cutouts = {}\n",
        "    cur_spot_cutouts = {}\n",
        "    cur_spot_off_cutouts = {}\n",
        "    for cutoutSize in cutoutsTable:\n",
        "        make_cutouts = cutoutsTable[cutoutSize]\n",
        "        cur_cutouts[cutoutSize] = make_cutouts(out)\n",
        "\n",
        "    if args.spot_prompts:\n",
        "        for cutoutSize in cutoutsTable:\n",
        "            cur_spot_cutouts[cutoutSize] = make_cutouts(out, spot=1)\n",
        "\n",
        "    if args.spot_prompts_off:\n",
        "        for cutoutSize in cutoutsTable:\n",
        "            cur_spot_off_cutouts[cutoutSize] = make_cutouts(out, spot=0)\n",
        "\n",
        "    for clip_model in args.clip_models:\n",
        "        perceptor = perceptors[clip_model]\n",
        "        cutoutSize = cutoutSizeTable[clip_model]\n",
        "        transient_pMs = []\n",
        "\n",
        "        if args.spot_prompts:\n",
        "            iii_s = perceptor.encode_image(normalize( cur_spot_cutouts[cutoutSize] )).float()\n",
        "            spotPms = spotPmsTable[clip_model]\n",
        "            for prompt in spotPms:\n",
        "                result.append(prompt(iii_s))\n",
        "\n",
        "        if args.spot_prompts_off:\n",
        "            iii_so = perceptor.encode_image(normalize( cur_spot_off_cutouts[cutoutSize] )).float()\n",
        "            spotOffPms = spotOffPmsTable[clip_model]\n",
        "            for prompt in spotOffPms:\n",
        "                result.append(prompt(iii_so))\n",
        "\n",
        "        pMs = pmsTable[clip_model]\n",
        "        iii = perceptor.encode_image(normalize( cur_cutouts[cutoutSize] )).float()\n",
        "        for prompt in pMs:\n",
        "            result.append(prompt(iii))\n",
        "\n",
        "        # If there are image prompts we make cutouts for those each time\n",
        "        # so that they line up with the current cutouts from augmentation\n",
        "        make_cutouts = cutoutsTable[cutoutSize]\n",
        "        pImages = pmsImageTable[clip_model]\n",
        "        for timg in pImages:\n",
        "            # note: this caches and reuses the transforms - a bit of a hack but it works\n",
        "\n",
        "            if args.image_prompt_shuffle:\n",
        "                # print(\"Disabling cached transforms\")\n",
        "                make_cutouts.transforms = None\n",
        "\n",
        "            # print(\"Building throwaway image prompts\")\n",
        "            # new way builds throwaway Prompts\n",
        "            batch = make_cutouts(timg)\n",
        "            embed = perceptor.encode_image(normalize(batch)).float()\n",
        "            if args.image_prompt_weight is not None:\n",
        "                transient_pMs.append(Prompt(embed, args.image_prompt_weight).to(device))\n",
        "            else:\n",
        "                transient_pMs.append(Prompt(embed).to(device))\n",
        "\n",
        "        for prompt in transient_pMs:\n",
        "            result.append(prompt(iii))\n",
        "\n",
        "    if args.enforce_palette_annealing and args.target_palette:\n",
        "        target_palette = torch.FloatTensor(args.target_palette).requires_grad_(False).to(device)\n",
        "        _pixels = cur_cutouts[cutoutSize].permute(0,2,3,1).reshape(-1,3)\n",
        "        palette_dists = torch.cdist(target_palette, _pixels, p=2)\n",
        "        best_guesses = palette_dists.argmin(axis=0)\n",
        "        diffs = _pixels - target_palette[best_guesses]\n",
        "        palette_loss = torch.mean( torch.norm( diffs, 2, dim=1 ) )*cur_cutouts[cutoutSize].shape[0]\n",
        "        result.append( palette_loss*cur_iteration/args.enforce_palette_annealing )\n",
        "\n",
        "    if args.enforce_smoothness and args.enforce_smoothness_type:\n",
        "        _pixels = cur_cutouts[cutoutSize].permute(0,2,3,1).reshape(-1,cur_cutouts[cutoutSize].shape[2],3)\n",
        "        gyr, gxr = torch.gradient(_pixels[:,:,0])\n",
        "        gyg, gxg = torch.gradient(_pixels[:,:,1])\n",
        "        gyb, gxb = torch.gradient(_pixels[:,:,2])\n",
        "        sharpness = torch.sqrt(gyr**2 + gxr**2+ gyg**2 + gxg**2 + gyb**2 + gxb**2)\n",
        "        if args.enforce_smoothness_type=='clipped':\n",
        "            sharpness = torch.clamp( sharpness, max=0.5 )\n",
        "        elif args.enforce_smoothness_type=='log':\n",
        "            sharpness = torch.log( torch.ones_like(sharpness)+sharpness )\n",
        "        sharpness = torch.mean( sharpness )\n",
        "\n",
        "        result.append( sharpness*cur_iteration/args.enforce_smoothness )\n",
        "\n",
        "    if args.enforce_saturation:\n",
        "        # based on the old \"percepted colourfulness\" heuristic from Hasler and Süsstrunk’s 2003 paper\n",
        "        # https://www.researchgate.net/publication/243135534_Measuring_Colourfulness_in_Natural_Images\n",
        "        _pixels = cur_cutouts[cutoutSize].permute(0,2,3,1).reshape(-1,3)\n",
        "        rg = _pixels[:,0]-_pixels[:,1]\n",
        "        yb = 0.5*(_pixels[:,0]+_pixels[:,1])-_pixels[:,2]\n",
        "        rg_std, rg_mean = torch.std_mean(rg)\n",
        "        yb_std, yb_mean = torch.std_mean(yb)\n",
        "        std_rggb = torch.sqrt(rg_std**2 + yb_std**2)\n",
        "        mean_rggb = torch.sqrt(rg_mean**2 + yb_mean**2)\n",
        "        colorfullness = std_rggb+.3*mean_rggb\n",
        "\n",
        "        result.append( -colorfullness*cur_iteration/args.enforce_saturation )\n",
        "\n",
        "    for cutoutSize in cutoutsTable:\n",
        "        # clear the transform \"cache\"\n",
        "        make_cutouts = cutoutsTable[cutoutSize]\n",
        "        make_cutouts.transforms = None\n",
        "\n",
        "    # main init_weight uses spherical loss\n",
        "    if args.target_images is not None and args.target_image_weight > 0:\n",
        "        if cur_anim_index is None:\n",
        "            cur_z_targets = z_targets\n",
        "        else:\n",
        "            cur_z_targets = [ z_targets[cur_anim_index] ]\n",
        "        for z_target in cur_z_targets:\n",
        "            f = drawer.get_z().reshape(1,-1)\n",
        "            f2 = z_target.reshape(1,-1)\n",
        "            cur_loss = spherical_dist_loss(f, f2) * args.target_image_weight\n",
        "            result.append(cur_loss)\n",
        "\n",
        "    if args.target_weight_pix:\n",
        "        if target_image_tensor is None:\n",
        "            print(\"OOPS TIT is 0\")\n",
        "        else:\n",
        "            cur_loss = F.l1_loss(out, target_image_tensor) * args.target_weight_pix\n",
        "            result.append(cur_loss)\n",
        "\n",
        "    if args.image_labels is not None:\n",
        "        for z_label in z_labels:\n",
        "            f = drawer.get_z().reshape(1,-1)\n",
        "            f2 = z_label.reshape(1,-1)\n",
        "            cur_loss = spherical_dist_loss(f, f2) * args.image_label_weight\n",
        "            result.append(cur_loss)\n",
        "\n",
        "    # main init_weight uses spherical loss\n",
        "    if args.init_weight:\n",
        "        f = drawer.get_z().reshape(1,-1)\n",
        "        f2 = z_orig.reshape(1,-1)\n",
        "        cur_loss = spherical_dist_loss(f, f2) * args.init_weight\n",
        "        result.append(cur_loss)\n",
        "\n",
        "    # these three init_weight variants offer mse_loss, mse_loss in pixel space, and cos loss\n",
        "    if args.init_weight_dist:\n",
        "        cur_loss = F.mse_loss(z, z_orig) * args.init_weight_dist / 2\n",
        "        result.append(cur_loss)\n",
        "\n",
        "    if args.init_weight_pix:\n",
        "        if init_image_tensor is None:\n",
        "            print(\"OOPS IIT is 0\")\n",
        "        else:\n",
        "            cur_loss = F.l1_loss(out, init_image_tensor) * args.init_weight_pix / 2\n",
        "            result.append(cur_loss)\n",
        "\n",
        "    if args.init_weight_cos:\n",
        "        f = drawer.get_z().reshape(1,-1)\n",
        "        f2 = z_orig.reshape(1,-1)\n",
        "        y = torch.ones_like(f[0])\n",
        "        cur_loss = F.cosine_embedding_loss(f, f2, y) * args.init_weight_cos\n",
        "        result.append(cur_loss)\n",
        "\n",
        "    if args.make_video:    \n",
        "        img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "        imageio.imwrite(f'./steps/frame_{cur_iteration:04d}.png', np.array(img))\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "zgf2Rf78Mocz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def re_average_z(args):\n",
        "    global gside_X, gside_Y\n",
        "    global device, drawer\n",
        "\n",
        "    # old_z = z.clone()\n",
        "    cur_z_image = drawer.to_image()\n",
        "    cur_z_image = cur_z_image.convert('RGB')\n",
        "    if overlay_image_rgba:\n",
        "        # print(\"applying overlay image\")\n",
        "        cur_z_image.paste(overlay_image_rgba, (0, 0), overlay_image_rgba)\n",
        "        cur_z_image.save(\"overlaid.png\")\n",
        "    cur_z_image = cur_z_image.resize((gside_X, gside_Y), Image.LANCZOS)\n",
        "    drawer.reapply_from_tensor(TF.to_tensor(cur_z_image).to(device).unsqueeze(0) * 2 - 1)\n"
      ],
      "metadata": {
        "id": "HhYDWUswM0i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "def train(args, cur_it):\n",
        "    global drawer;\n",
        "    for opt in opts:\n",
        "        # opt.zero_grad(set_to_none=True)\n",
        "        opt.zero_grad()\n",
        "\n",
        "    for i in range(args.batches):\n",
        "        lossAll = ascend_txt(args)\n",
        "\n",
        "        if i == 0 and cur_it % args.save_every == 0:\n",
        "            checkin(args, cur_it, lossAll)\n",
        "\n",
        "        loss = sum(lossAll)\n",
        "        loss.backward()\n",
        "\n",
        "    for opt in opts:\n",
        "        opt.step()\n",
        "\n",
        "    if args.overlay_every and cur_it != 0 and \\\n",
        "        (cur_it % (args.overlay_every + args.overlay_offset)) == 0:\n",
        "        re_average_z(args)\n",
        "\n",
        "    drawer.clip_z()    \n",
        "\n",
        "imagenet_templates = [\n",
        "    \"itap of a {}.\",\n",
        "    \"a bad photo of the {}.\",\n",
        "    \"a origami {}.\",\n",
        "    \"a photo of the large {}.\",\n",
        "    \"a {} in a video game.\",\n",
        "    \"art of the {}.\",\n",
        "    \"a photo of the small {}.\",\n",
        "]"
      ],
      "metadata": {
        "id": "IsJ0s8rCM8Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_run(args):\n",
        "    global cur_iteration, cur_anim_index\n",
        "    global anim_cur_zs, anim_next_zs, anim_output_files\n",
        "\n",
        "    cur_iteration = 0\n",
        "\n",
        "    if args.animation_dir is not None:\n",
        "        # we already have z_targets. setup some sort of global ring\n",
        "        # we need something like\n",
        "        # copies of all the current z's (they can all start off all as copies)\n",
        "        # a list of all the output filenames\n",
        "        #\n",
        "        if not os.path.exists(args.animation_dir):\n",
        "            os.mkdir(args.animation_dir)\n",
        "        filelist = real_glob(args.target_images)\n",
        "        num_anim_frames = len(filelist)\n",
        "        for target_image in filelist:\n",
        "            basename = os.path.basename(target_image)\n",
        "            target_output = os.path.join(args.animation_dir, basename)\n",
        "            anim_output_files.append(target_output)\n",
        "        for i in range(num_anim_frames):\n",
        "            cur_z = drawer.get_z_copy()\n",
        "            anim_cur_zs.append(cur_z)\n",
        "            anim_next_zs.append(None)\n",
        "\n",
        "        step_iteration = 0\n",
        "\n",
        "        with tqdm() as pbar:\n",
        "            while True:\n",
        "                cur_images = []\n",
        "                for i in range(num_anim_frames):\n",
        "                    # do merge frames here from cur->next when we are ready to be fancy\n",
        "                    cur_anim_index = i\n",
        "                    # anim_cur_zs[cur_anim_index] = anim_next_zs[cur_anim_index]\n",
        "                    cur_iteration = step_iteration\n",
        "                    drawer.set_z(anim_cur_zs[cur_anim_index])\n",
        "                    for j in range(args.save_every):\n",
        "                        train(args, cur_iteration)\n",
        "                        cur_iteration += 1\n",
        "                        pbar.update()\n",
        "                    # anim_next_zs[cur_anim_index] = drawer.get_z_copy()\n",
        "                    cur_images.append(drawer.to_image())\n",
        "                step_iteration = step_iteration + args.save_every\n",
        "                if step_iteration >= args.iterations:\n",
        "                    break\n",
        "                # compute the next round of cur_zs here from all the next_zs\n",
        "                for i in range(num_anim_frames):\n",
        "                    prev_i = (i + num_anim_frames - 1) % num_anim_frames\n",
        "                    base_image = cur_images[i].copy()\n",
        "                    prev_image = cur_images[prev_i].copy().convert('RGBA')\n",
        "                    prev_image.putalpha(args.animation_alpha)\n",
        "                    base_image.paste(prev_image, (0, 0), prev_image)\n",
        "                    # base_image.save(f\"overlaid_{i:02d}.png\")\n",
        "                    drawer.reapply_from_tensor(TF.to_tensor(base_image).to(device).unsqueeze(0) * 2 - 1)\n",
        "                    anim_cur_zs[i] = drawer.get_z_copy()\n",
        "    else:\n",
        "        try:\n",
        "            with tqdm() as pbar:\n",
        "                while True:\n",
        "                    try:\n",
        "                        train(args, cur_iteration)\n",
        "                        if cur_iteration == args.iterations:\n",
        "                            break\n",
        "                        cur_iteration += 1\n",
        "                        pbar.update()\n",
        "                    except RuntimeError as e:\n",
        "                        print(\"Oops: runtime error: \", e)\n",
        "                        print(\"Try reducing --num-cuts to save memory\")\n",
        "                        raise e\n",
        "        except KeyboardInterrupt:\n",
        "            pass\n",
        "\n",
        "    if args.make_video:\n",
        "        do_video(args)"
      ],
      "metadata": {
        "id": "EhT2S3EZNSW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_video(args):\n",
        "    global cur_iteration\n",
        "\n",
        "    # Video generation\n",
        "    init_frame = 1 # This is the frame where the video will start\n",
        "    last_frame = cur_iteration # You can change to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
        "\n",
        "    min_fps = 10\n",
        "    max_fps = 60\n",
        "\n",
        "    total_frames = last_frame-init_frame\n",
        "\n",
        "    length = 15 # Desired time of the video in seconds\n",
        "\n",
        "    frames = [f'./steps/frame_{i:04d}.png' for i in range(init_frame,last_frame)]\n",
        "    frames.sort()\n",
        "\n",
        "    #fps = last_frame/10\n",
        "    fps = np.clip(total_frames/length,min_fps,max_fps)\n",
        "    clip = ImageSequenceClip(frames, fps = fps)\n",
        "    \n",
        "    import re\n",
        "    output_file = re.compile('\\.png$').sub('.mp4', args.output)\n",
        "    clip.write_videofile(output_file)\n",
        "\n",
        "# this dictionary is used for settings in the notebook\n",
        "global_clipit_settings = {}\n"
      ],
      "metadata": {
        "id": "_KxTIVAYNe3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_parser():\n",
        "    # Create the parser\n",
        "    vq_parser = argparse.ArgumentParser(description='Image generation using VQGAN+CLIP')\n",
        "\n",
        "    # Add the arguments\n",
        "    vq_parser.add_argument(\"-p\",    \"--prompts\", type=str, help=\"Text prompts\", default=[], dest='prompts')\n",
        "    vq_parser.add_argument(\"-sp\",   \"--spot\", type=str, help=\"Spot Text prompts\", default=[], dest='spot_prompts')\n",
        "    vq_parser.add_argument(\"-spo\",  \"--spot_off\", type=str, help=\"Spot off Text prompts\", default=[], dest='spot_prompts_off')\n",
        "    vq_parser.add_argument(\"-spf\",  \"--spot_file\", type=str, help=\"Custom spot file\", default=None, dest='spot_file')\n",
        "    vq_parser.add_argument(\"-l\",    \"--labels\", type=str, help=\"ImageNet labels\", default=[], dest='labels')\n",
        "    vq_parser.add_argument(\"-ip\",   \"--image_prompts\", type=str, help=\"Image prompts\", default=[], dest='image_prompts')\n",
        "    vq_parser.add_argument(\"-ipw\",  \"--image_prompt_weight\", type=float, help=\"Weight for image prompt\", default=None, dest='image_prompt_weight')\n",
        "    vq_parser.add_argument(\"-ips\",  \"--image_prompt_shuffle\", type=bool, help=\"Shuffle image prompts\", default=False, dest='image_prompt_shuffle')\n",
        "    vq_parser.add_argument(\"-il\",   \"--image_labels\", type=str, help=\"Image prompts\", default=None, dest='image_labels')\n",
        "    vq_parser.add_argument(\"-ilw\",  \"--image_label_weight\", type=float, help=\"Weight for image prompt\", default=1.0, dest='image_label_weight')\n",
        "    vq_parser.add_argument(\"-i\",    \"--iterations\", type=int, help=\"Number of iterations\", default=None, dest='iterations')\n",
        "    vq_parser.add_argument(\"-se\",   \"--save_every\", type=int, help=\"Save image iterations\", default=10, dest='save_every')\n",
        "    vq_parser.add_argument(\"-de\",   \"--display_every\", type=int, help=\"Display image iterations\", default=20, dest='display_every')\n",
        "    vq_parser.add_argument(\"-ove\",  \"--overlay_every\", type=int, help=\"Overlay image iterations\", default=None, dest='overlay_every')\n",
        "    vq_parser.add_argument(\"-ovo\",  \"--overlay_offset\", type=int, help=\"Overlay image iteration offset\", default=0, dest='overlay_offset')\n",
        "    vq_parser.add_argument(\"-ovi\",  \"--overlay_image\", type=str, help=\"Overlay image (if not init)\", default=None, dest='overlay_image')\n",
        "    vq_parser.add_argument(\"-qua\",  \"--quality\", type=str, help=\"draft, normal, best\", default=\"normal\", dest='quality')\n",
        "    vq_parser.add_argument(\"-asp\",  \"--aspect\", type=str, help=\"widescreen, square\", default=\"widescreen\", dest='aspect')\n",
        "    vq_parser.add_argument(\"-ezs\",  \"--ezsize\", type=str, help=\"small, medium, large\", default=None, dest='ezsize')\n",
        "    vq_parser.add_argument(\"-sca\",  \"--scale\", type=float, help=\"scale (instead of ezsize)\", default=None, dest='scale')\n",
        "    vq_parser.add_argument(\"-ova\",  \"--overlay_alpha\", type=int, help=\"Overlay alpha (0-255)\", default=None, dest='overlay_alpha')    \n",
        "    vq_parser.add_argument(\"-s\",    \"--size\", nargs=2, type=int, help=\"Image size (width height)\", default=None, dest='size')\n",
        "    vq_parser.add_argument(\"-ps\",   \"--pixel_size\", nargs=2, type=int, help=\"Pixel size (width height)\", default=None, dest='pixel_size')\n",
        "    vq_parser.add_argument(\"-psc\",  \"--pixel_scale\", type=float, help=\"Pixel scale\", default=None, dest='pixel_scale')\n",
        "    vq_parser.add_argument(\"-ii\",   \"--init_image\", type=str, help=\"Initial image\", default=None, dest='init_image')\n",
        "    vq_parser.add_argument(\"-iia\",  \"--init_image_alpha\", type=int, help=\"Init image alpha (0-255)\", default=200, dest='init_image_alpha')\n",
        "    vq_parser.add_argument(\"-in\",   \"--init_noise\", type=str, help=\"Initial noise image (pixels or gradient)\", default=\"pixels\", dest='init_noise')\n",
        "    vq_parser.add_argument(\"-ti\",   \"--target_images\", type=str, help=\"Target images\", default=None, dest='target_images')\n",
        "    vq_parser.add_argument(\"-tiw\",  \"--target_image_weight\", type=float, help=\"Target images weight\", default=1.0, dest='target_image_weight')\n",
        "    vq_parser.add_argument(\"-twp\",  \"--target_weight_pix\", type=float, help=\"Target weight pix loss\", default=0., dest='target_weight_pix')\n",
        "    vq_parser.add_argument(\"-anim\", \"--animation_dir\", type=str, help=\"Animation output dir\", default=None, dest='animation_dir')    \n",
        "    vq_parser.add_argument(\"-ana\",  \"--animation_alpha\", type=int, help=\"Forward blend for consistency\", default=128, dest='animation_alpha')\n",
        "    vq_parser.add_argument(\"-iw\",   \"--init_weight\", type=float, help=\"Initial weight (main=spherical)\", default=None, dest='init_weight')\n",
        "    vq_parser.add_argument(\"-iwd\",  \"--init_weight_dist\", type=float, help=\"Initial weight dist loss\", default=0., dest='init_weight_dist')\n",
        "    vq_parser.add_argument(\"-iwc\",  \"--init_weight_cos\", type=float, help=\"Initial weight cos loss\", default=0., dest='init_weight_cos')\n",
        "    vq_parser.add_argument(\"-iwp\",  \"--init_weight_pix\", type=float, help=\"Initial weight pix loss\", default=0., dest='init_weight_pix')\n",
        "    vq_parser.add_argument(\"-m\",    \"--clip_models\", type=str, help=\"CLIP model\", default=None, dest='clip_models')\n",
        "    vq_parser.add_argument(\"-vqgan\", \"--vqgan_model\", type=str, help=\"VQGAN model\", default='imagenet_f16_16384', dest='vqgan_model')\n",
        "    vq_parser.add_argument(\"-conf\", \"--vqgan_config\", type=str, help=\"VQGAN config\", default=None, dest='vqgan_config')\n",
        "    vq_parser.add_argument(\"-ckpt\", \"--vqgan_checkpoint\", type=str, help=\"VQGAN checkpoint\", default=None, dest='vqgan_checkpoint')\n",
        "    vq_parser.add_argument(\"-nps\",  \"--noise_prompt_seeds\", nargs=\"*\", type=int, help=\"Noise prompt seeds\", default=[], dest='noise_prompt_seeds')\n",
        "    vq_parser.add_argument(\"-npw\",  \"--noise_prompt_weights\", nargs=\"*\", type=float, help=\"Noise prompt weights\", default=[], dest='noise_prompt_weights')\n",
        "    vq_parser.add_argument(\"-lr\",   \"--learning_rate\", type=float, help=\"Learning rate\", default=0.2, dest='learning_rate')\n",
        "    vq_parser.add_argument(\"-cuts\", \"--num_cuts\", type=int, help=\"Number of cuts\", default=None, dest='num_cuts')\n",
        "    vq_parser.add_argument(\"-bats\", \"--batches\", type=int, help=\"How many batches of cuts\", default=1, dest='batches')\n",
        "    vq_parser.add_argument(\"-cutp\", \"--cut_power\", type=float, help=\"Cut power\", default=1., dest='cut_pow')\n",
        "    vq_parser.add_argument(\"-sd\",   \"--seed\", type=int, help=\"Seed\", default=None, dest='seed')\n",
        "    vq_parser.add_argument(\"-opt\",  \"--optimiser\", type=str, help=\"Optimiser (Adam, AdamW, Adagrad, Adamax, DiffGrad, AdamP or RAdam)\", default='Adam', dest='optimiser')\n",
        "    vq_parser.add_argument(\"-o\",    \"--output\", type=str, help=\"Output file\", default=\"output.png\", dest='output')\n",
        "    vq_parser.add_argument(\"-vid\",  \"--video\", type=bool, help=\"Create video frames?\", default=False, dest='make_video')\n",
        "    vq_parser.add_argument(\"-d\",    \"--deterministic\", type=bool, help=\"Enable cudnn.deterministic?\", default=False, dest='cudnn_determinism')\n",
        "    vq_parser.add_argument(\"-cd\",   \"--use_clipdraw\", type=bool, help=\"Use clipdraw\", default=False, dest='use_clipdraw')\n",
        "    vq_parser.add_argument(\"-st\",   \"--strokes\", type=int, help=\"clipdraw strokes\", default=1024, dest='strokes')\n",
        "    vq_parser.add_argument(\"-pd\",   \"--use_pixeldraw\", type=bool, help=\"Use pixeldraw\", default=False, dest='use_pixeldraw')\n",
        "    vq_parser.add_argument(\"-mo\",   \"--do_mono\", type=bool, help=\"Monochromatic\", default=False, dest='do_mono')\n",
        "    vq_parser.add_argument(\"-epw\",  \"--enforce_palette_annealing\", type=int, help=\"enforce palette annealing, 0 -- skip\", default=5000, dest='enforce_palette_annealing')\n",
        "    vq_parser.add_argument(\"-tp\",   \"--target_palette\", type=str, help=\"target palette\", default=None, dest='target_palette')\n",
        "    vq_parser.add_argument(\"-esw\",  \"--enforce_smoothness\", type=int, help=\"enforce smoothness, 0 -- skip\", default=0, dest='enforce_smoothness')\n",
        "    vq_parser.add_argument(\"-est\",  \"--enforce_smoothness_type\", type=str, help=\"enforce smoothness type: default/clipped/log\", default='default', dest='enforce_smoothness_type')\n",
        "    vq_parser.add_argument(\"-ecw\",  \"--enforce_saturation\", type=int, help=\"enforce saturation, 0 -- skip\", default=0, dest='enforce_saturation')\n",
        "\n",
        "    return vq_parser\n",
        "\n",
        "square_size = [144, 144]\n",
        "widescreen_size = [160, 90]  # at the small size this becomes 192,112"
      ],
      "metadata": {
        "id": "LLVN9OLwNs2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### PALETTE SECTION ##########\n",
        "\n",
        "# canonical interpolation function, like https://p5js.org/reference/#/p5/map\n",
        "def map_number(n, start1, stop1, start2, stop2):\n",
        "  return ((n-start1)/(stop1-start1))*(stop2-start2)+start2;\n"
      ],
      "metadata": {
        "id": "gq300a8MN1RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here are examples of what can be parsed\n",
        "# white   (16 color black to white ramp)\n",
        "# red     (16 color black to red ramp)\n",
        "# rust\\8  (8 color black to rust ramp)\n",
        "# red->rust         (16 color red to rust ramp)\n",
        "# red->#ff0000      (16 color red to yellow ramp)\n",
        "# red->#ff0000\\20   (20 color red to yellow ramp)\n",
        "# black->red->white (16 color black/red/white ramp)\n",
        "# [black, red, #ff0000] (three colors)\n",
        "# red->white;blue->yellow (32 colors across two ramps of 16)\n",
        "# red;blue;yellow         (48 colors from combining 3 ramps)\n",
        "# red\\8;blue->yellow\\8    (16 colors from combining 2 ramps)\n",
        "# red->yellow;[black]     (16 colors from ramp and also black)\n",
        "#\n",
        "# TODO: maybe foo.jpg, foo.json, foo.png, foo.asc\n",
        "def get_single_rgb(s):\n",
        "    palette_lookups = {\n",
        "        \"pixel_green\":     [0.44, 1.00, 0.53],\n",
        "        \"pixel_orange\":    [1.00, 0.80, 0.20],\n",
        "        \"pixel_blue\":      [0.44, 0.53, 1.00],\n",
        "        \"pixel_red\":       [1.00, 0.53, 0.44],\n",
        "        \"pixel_grayscale\": [1.00, 1.00, 1.00],\n",
        "    }\n",
        "    if s in palette_lookups:\n",
        "        rgb = palette_lookups[s]\n",
        "    elif s[:4] == \"mat:\":\n",
        "        rgb = matplotlib.colors.to_rgb(s[4:])\n",
        "    elif matplotlib.colors.is_color_like(f\"xkcd:{s}\"):\n",
        "        rgb = matplotlib.colors.to_rgb(f\"xkcd:{s}\")\n",
        "    else:\n",
        "        rgb = matplotlib.colors.to_rgb(s)\n",
        "    return rgb"
      ],
      "metadata": {
        "id": "baSWkyZmN77v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_colors(colors, num_steps):\n",
        "    index_episilon = 1e-6;\n",
        "    pal = []\n",
        "    num_colors = len(colors)\n",
        "    for n in range(num_steps):\n",
        "        cur_float_index = map_number(n, 0, num_steps-1, 0, num_colors-1)\n",
        "        cur_int_index = int(cur_float_index)\n",
        "        cur_float_offset = cur_float_index - cur_int_index\n",
        "        if(cur_float_offset < index_episilon or (1.0-cur_float_offset) < index_episilon):\n",
        "            # debug print(n, \"->\", cur_int_index)\n",
        "            pal.append(colors[cur_int_index])\n",
        "        else:\n",
        "            # debug print(n, num_steps, num_colors, cur_float_index, cur_int_index, cur_float_offset)\n",
        "            rgb1 = colors[cur_int_index]\n",
        "            rgb2 = colors[cur_int_index+1]\n",
        "            r = map_number(cur_float_offset, 0, 1, rgb1[0], rgb2[0])\n",
        "            g = map_number(cur_float_offset, 0, 1, rgb1[1], rgb2[1])\n",
        "            b = map_number(cur_float_offset, 0, 1, rgb1[2], rgb2[2])\n",
        "            pal.append([r, g, b])\n",
        "    return pal\n"
      ],
      "metadata": {
        "id": "G3IkSbEDOCGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rgb_range(s):\n",
        "    # get the list that defines the range\n",
        "    if s.find('->') > 0:\n",
        "        parts = s.split('->')\n",
        "    else:\n",
        "        parts = [\"black\", s]\n",
        "\n",
        "    # look for a number of parts at the end\n",
        "    if parts[-1].find('\\\\') > 0:\n",
        "        colname, steps = parts[-1].split('\\\\')\n",
        "        parts[-1] = colname\n",
        "        num_steps = int(steps)\n",
        "    else:\n",
        "        num_steps = 16\n",
        "\n",
        "    colors = [get_single_rgb(s) for s in parts]\n",
        "    #debug print(\"We have colors: \", colors)\n",
        "\n",
        "    pal = expand_colors(colors, num_steps)\n",
        "    return pal\n"
      ],
      "metadata": {
        "id": "sAOYA9VqOImb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def palette_from_section(s):\n",
        "    s = s.strip()\n",
        "    if s[0] == '[':\n",
        "        # look for a number of parts at the end\n",
        "        if s.find('\\\\') > 0:\n",
        "            col_list, steps = s.split('\\\\')\n",
        "            s = col_list\n",
        "            num_steps = int(steps)\n",
        "        else:\n",
        "            num_steps = None\n",
        "\n",
        "        chunks = s[1:-1].split(\",\")\n",
        "        # chunks = [s.strip().tolower() for c in chunks]\n",
        "        pal = [get_single_rgb(c.strip()) for c in chunks]\n",
        "\n",
        "        if num_steps is not None:\n",
        "            pal = expand_colors(pal, num_steps)\n",
        "\n",
        "        return pal\n",
        "    else:\n",
        "        return get_rgb_range(s)"
      ],
      "metadata": {
        "id": "RUcCBe6aON0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def palette_from_string(s):\n",
        "    s = s.strip()\n",
        "    pal = []\n",
        "    chunks = s.split(';')\n",
        "    for c in chunks:\n",
        "        pal = pal + palette_from_section(c)\n",
        "    return pal\n"
      ],
      "metadata": {
        "id": "3EEHpnq_OTRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_args(vq_parser, namespace=None):\n",
        "    global global_aspect_width\n",
        "    global cur_iteration, cur_anim_index, anim_output_files, anim_cur_zs, anim_next_zs;\n",
        "    global global_spot_file\n",
        "\n",
        "    if namespace == None:\n",
        "      # command line: use ARGV to get args\n",
        "      args = vq_parser.parse_args()\n",
        "    else:\n",
        "      # notebook, ignore ARGV and use dictionary instead\n",
        "      args = vq_parser.parse_args(args=[], namespace=namespace)\n",
        "\n",
        "    if args.cudnn_determinism:\n",
        "       torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    quality_to_clip_models_table = {\n",
        "        'draft': 'ViT-B/32',\n",
        "        'normal': 'ViT-B/32,ViT-B/16',\n",
        "        'better': 'RN50,ViT-B/32,ViT-B/16',\n",
        "        'best': 'RN50x4,ViT-B/32,ViT-B/16'\n",
        "    }\n",
        "    quality_to_iterations_table = {\n",
        "        'draft': 200,\n",
        "        'normal': 350,\n",
        "        'better': 500,\n",
        "        'best': 500\n",
        "    }\n",
        "    quality_to_scale_table = {\n",
        "        'draft': 1,\n",
        "        'normal': 2,\n",
        "        'better': 3,\n",
        "        'best': 4\n",
        "    }\n",
        "    # this should be replaced with logic that does somethings\n",
        "    # smart based on available memory (eg: size, num_models, etc)\n",
        "    quality_to_num_cuts_table = {\n",
        "        'draft': 40,\n",
        "        'normal': 40,\n",
        "        'better': 40,\n",
        "        'best': 40\n",
        "    }\n",
        "\n",
        "    if args.quality not in quality_to_clip_models_table:\n",
        "        print(\"Qualitfy setting not understood, aborting -> \", args.quality)\n",
        "        exit(1)\n",
        "\n",
        "    if args.clip_models is None:\n",
        "        args.clip_models = quality_to_clip_models_table[args.quality]\n",
        "    if args.iterations is None:\n",
        "        args.iterations = quality_to_iterations_table[args.quality]\n",
        "    if args.num_cuts is None:\n",
        "        args.num_cuts = quality_to_num_cuts_table[args.quality]\n",
        "    if args.ezsize is None and args.scale is None:\n",
        "        args.scale = quality_to_scale_table[args.quality]\n",
        "\n",
        "    size_to_scale_table = {\n",
        "        'small': 1,\n",
        "        'medium': 2,\n",
        "        'large': 4\n",
        "    }\n",
        "    aspect_to_size_table = {\n",
        "        'square': [150, 150],\n",
        "        'widescreen': [160, 90],\n",
        "        'portrait': [128, 160]\n",
        "    }\n",
        "\n",
        "    if args.size is not None:\n",
        "        global_aspect_width = args.size[0] / args.size[1]\n",
        "    elif args.aspect == \"widescreen\":\n",
        "        global_aspect_width = 16/9\n",
        "    else:\n",
        "        global_aspect_width = 1\n",
        "\n",
        "    # determine size if not set\n",
        "    if args.size is None:\n",
        "        size_scale = args.scale\n",
        "        if size_scale is None:\n",
        "            if args.ezsize in size_to_scale_table:\n",
        "                size_scale = size_to_scale_table[args.ezsize]\n",
        "            else:\n",
        "                print(\"EZ Size not understood, aborting -> \", args.ezsize)\n",
        "                exit(1)\n",
        "        if args.aspect in aspect_to_size_table:\n",
        "            base_size = aspect_to_size_table[args.aspect]\n",
        "            base_width = int(size_scale * base_size[0])\n",
        "            base_height = int(size_scale * base_size[1])\n",
        "            args.size = [base_width, base_height]\n",
        "        else:\n",
        "            print(\"aspect not understood, aborting -> \", args.aspect)\n",
        "            exit(1)\n",
        "\n",
        "    if args.init_noise.lower() == \"none\":\n",
        "        args.init_noise = None\n",
        "\n",
        "    # Split text prompts using the pipe character\n",
        "    if args.prompts:\n",
        "        args.prompts = [phrase.strip() for phrase in args.prompts.split(\"|\")]\n",
        "\n",
        "    # Split text prompts using the pipe character\n",
        "    if args.spot_prompts:\n",
        "        args.spot_prompts = [phrase.strip() for phrase in args.spot_prompts.split(\"|\")]\n",
        "\n",
        "    # Split text prompts using the pipe character\n",
        "    if args.spot_prompts_off:\n",
        "        args.spot_prompts_off = [phrase.strip() for phrase in args.spot_prompts_off.split(\"|\")]\n",
        "\n",
        "    # Split text labels using the pipe character\n",
        "    if args.labels:\n",
        "        args.labels = [phrase.strip() for phrase in args.labels.split(\"|\")]\n",
        "\n",
        "    # Split target images using the pipe character\n",
        "    if args.image_prompts:\n",
        "        args.image_prompts = args.image_prompts.split(\"|\")\n",
        "        args.image_prompts = [image.strip() for image in args.image_prompts]\n",
        "\n",
        "    if args.target_palette is not None:\n",
        "        args.target_palette = palette_from_string(args.target_palette)\n",
        "\n",
        "    if args.overlay_every is not None and args.overlay_every <= 0:\n",
        "        args.overlay_every = None\n",
        "\n",
        "    clip_models = args.clip_models.split(\",\")\n",
        "    args.clip_models = [model.strip() for model in clip_models]\n",
        "\n",
        "    # Make video steps directory\n",
        "    if args.make_video:\n",
        "        if not os.path.exists('steps'):\n",
        "            os.mkdir('steps')\n",
        "\n",
        "    # reset global animation variables\n",
        "    cur_iteration=None\n",
        "    cur_anim_index=None\n",
        "    anim_output_files=[]\n",
        "    anim_cur_zs=[]\n",
        "    anim_next_zs=[]\n",
        "\n",
        "    global_spot_file = args.spot_file\n",
        "\n",
        "    return args"
      ],
      "metadata": {
        "id": "8mGhPHyjOcln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_settings():\n",
        "    global global_clipit_settings\n",
        "    global_clipit_settings = {}"
      ],
      "metadata": {
        "id": "Wr2opcfgTkpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_settings(**kwargs):\n",
        "    global global_clipit_settings\n",
        "    for k, v in kwargs.items():\n",
        "        if v is None:\n",
        "            # just remove the key if it is there\n",
        "            global_clipit_settings.pop(k, None)\n",
        "        else:\n",
        "            global_clipit_settings[k] = v"
      ],
      "metadata": {
        "id": "0bHVsBRrTljd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_settings():\n",
        "    global global_clipit_settings\n",
        "    settingsDict = None\n",
        "    vq_parser = setup_parser()\n",
        "\n",
        "    if len(global_clipit_settings) > 0:\n",
        "        # check for any bogus entries in the settings\n",
        "        dests = [d.dest for d in vq_parser._actions]\n",
        "        for k in global_clipit_settings:\n",
        "            if not k in dests:\n",
        "                raise ValueError(f\"Requested setting not found, aborting: {k}={global_clipit_settings[k]}\")\n",
        "\n",
        "        # convert dictionary to easyDict\n",
        "        # which can be used as an argparse namespace instead\n",
        "        # settingsDict = easydict.EasyDict(global_clipit_settings)\n",
        "        settingsDict = SimpleNamespace(**global_clipit_settings)\n",
        "\n",
        "    settings = process_args(vq_parser, settingsDict)\n",
        "    return settings\n"
      ],
      "metadata": {
        "id": "v1N6UZStTrLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    settings = apply_settings()    \n",
        "    do_init(settings)\n",
        "    do_run(settings)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "# To reset settings to default\n",
        "clipit.reset_settings()\n",
        "\n",
        "# You can use \"|\" to separate multiple prompts\n",
        "prompts = \"underwater city\"\n",
        "\n",
        "# You can trade off speed for quality: draft, normal, better, best\n",
        "quality = \"normal\"\n",
        "\n",
        "# Aspect ratio: widescreen, square\n",
        "aspect = \"widescreen\"\n",
        "\n",
        "# Add settings\n",
        "clipit.add_settings(prompts=prompts, quality=quality, aspect=aspect)\n",
        "\n",
        "# Apply these settings and run\n",
        "settings = clipit.apply_settings()\n",
        "clipit.do_init(settings)\n",
        "cliptit.do_run(settings)"
      ],
      "metadata": {
        "id": "JR9KSTYZTwpJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}